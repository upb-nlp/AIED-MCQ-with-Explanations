
## Performance Evaluation

We used a dataset from the iSTART Intelligent Tutoring System [Perret et al., 2017](#perret2017) for K-12 education to assess the performance of our method. We considered a subset with 55 texts covering a range of subjects, ranging from history and geography to science and technology. Each text contains an average of 500 words (approximately 30 sentences).

### Baselines

To evaluate the effectiveness of our approach to MCQ generation, we conducted a comparative analysis using questions generated using three alternative methods. By generating questions with each of these methods, we aimed to provide a detailed assessment of our model's performance in relation to both automated and human-generated standards.

**Without explanations.** We analyze the impact of incorporating reasoning and explanations during training and inference on the quality of generated questions. Specifically, we conducted an ablation study by finetuning the model to generate only the question, answer, and distractors; thus, we excluded the reasoning for the correct answer and explanations of misconceptions about distractors. The training process followed the same format and hyperparameter settings as the proposed model, with the only difference being the omission of reasoning and explanation components in the prompt.

**GPT-4o.** As an industry-standard baseline, we employed GPT-4o, a proprietary model with inference costs. We prompted GPT-4o using the same structured input as our model to generate a set of MCQs, including the question, reasoning for the correct answers, the correct answers themselves, common misconceptions, and corresponding distractors.

**Human questions.** The dataset included MCQs serving as a reference point for assessing the quality of automatically generated MCQs.

### Evaluation Setup

We employed a pairwise comparison approach to evaluate the quality of MCQs generated by different methods (i.e., our proposed model with explanations, the simple model without explanations, GPT-4o, and human-authored MCQs), where human annotators assessed pairs of items from different sources. Pairwise comparison is a widely used evaluation technique, especially in tasks requiring subjective quality judgments. It has been frequently applied in automatic evaluation settings, where LLMs serve as judges [Qin et al., 2024](#qin2024), [Liusie et al., 2024](#liusie2024), arguing for its effectiveness in preference-based assessments.

Compared to absolute rating scales, pairwise comparisons provide clearer results by directly revealing which method performs better while also allowing for ties when questions of different topics are of similar quality. It reduces cognitive strain on annotators, as judging between two items is often more intuitive and consistent than assigning independent scores.

We conducted the human evaluation study with 5 annotators to assess the quality of MCQs generated by different methods. All annotators speak English as a second language and have been involved in educational activities with college students. Each annotator was tasked with evaluating 15 texts, performing 30 pairwise comparisons per text.

All annotators initially evaluated five texts (n = 150 pairs) to ensure that the evaluators understood the task, and the agreement was calculated using percent agreement and Conger's Kappa [Conger, 1980](#conger1980), a generalization of Cohen's Kappa for multiple raters. The agreement was computed both with and without considering ties. Percent agreement with ties (n = 150 pairs) was 49% with a Conger's Kappa of 0.19 [C.I. 95% 0.14 - 0.24], categorized as slight [Landis & Koch, 1977](#landis1977). Percent agreement without ties (n = 65 pairs) was 64% with a Conger's Kappa of 0.29 [C.I. 95% 0.17-0.41], categorized as fair [Landis & Koch, 1977](#landis1977). According to Amidei et al. [2018](#amidei2018), this type of agreement is very common in human evaluation of automatically generated text. Furthermore, we aimed to enhance the ecological validity of the evaluation by comparing the methods in a setting as close as possible to a real-world scenario, where subjectivity adjustments are typically not made. In this context, the calibration phase focused on ensuring that each evaluator clearly understood the instructions rather than striving for consistency in evaluation outcomes across raters. This approach enabled evaluators to apply their individual interpretations while maintaining a shared goal, which reflects real-life situations. After calibration, each annotator proceeded with their individual assignments, annotating 10 unique texts.

Each text contained 30 pairwise comparisons, generated as follows: each method (our proposed model with explanations, our approach without explanations, GPT-4O, and human-authored MCQs) contributed 5 questions per text. These questions were then systematically paired, ensuring that each comparison featured questions from different methods. Specifically, each question from one method was paired with questions from the other three methods, distributing the comparisons evenly and maximizing cross-method evaluation.

During annotation, evaluators were instructed to select the better MCQ or declare a tie. Their decision-making was guided by instructions focused on determining whether the generated questions and answers met four quality criteria:  
a) whether the correct answer (always listed first) is the only correct option,  
b) whether the question and options are based on information from the text,  
c) whether the MCQ is not trivial, and  
d) whether the question assesses understanding rather than simple recall.  

The evaluators were instructed to select the question that met the most criteria and, in cases of a tie, to make a subjective choice, using the "tie" option only if the decision was particularly difficult.

---

### References

- <a name="perret2017"></a>Perret et al. (2017)
- <a name="qin2024"></a>Qin et al. (2024)
- <a name="liusie2024"></a>Liusie et al. (2024)
- <a name="conger1980"></a>Conger (1980)
- <a name="landis1977"></a>Landis & Koch (1977)
- <a name="amidei2018"></a>Amidei et al. (2018)
